# RAG 知识库数据分析与技术笔记

> 更新日期: 2026-02-11

---

## 1. 文档规模统计

对 `input_data/` 目录下的 PDF 文件进行字符和页数统计（使用 PyMuPDF 提取纯文本）：

| 文件                                                             |       页数 |         字符数 |  ≈ 英文词数 |
| ---------------------------------------------------------------- | ---------: | -------------: | ----------: |
| fcug.pdf                                                         |      1,009 |      1,831,848 |      ~30 万 |
| fusion compiler application options and attributes 24.09.sp3.pdf |      6,027 |     11,660,304 |     ~194 万 |
| fusion compiler error messages.pdf                               |      9,161 |     14,297,422 |     ~238 万 |
| fusion compiler tool commands.pdf                                |      6,475 |     11,305,585 |     ~188 万 |
| fusion compiler user guide -24.09.sp3.pdf                        |      1,086 |      1,961,792 |      ~33 万 |
| fusion compiler variables and attributes 23.12.pdf               |      2,259 |      3,364,599 |      ~56 万 |
| ptug+.pdf                                                        |      1,348 |      2,414,272 |      ~40 万 |
| **合计**                                                         | **27,365** | **46,835,822** | **~780 万** |

### 字符 vs 字（词）

- **字符 (character)**：每个字母、数字、空格、标点各计 1 个。`"Hello World"` = 11 字符
- **字 / 词 (word)**：以空格分隔的单词数。`"Hello World"` = 2 词
- 英文中 **1 词 ≈ 5~6 字符**（含空格）
- NotebookLM 的"50 万字"限制指的是 50 万 **words**，上表中有 4 个文件远超此限制

### 快速统计命令

```powershell
# 单个文件
py -c "import pymupdf; doc=pymupdf.open('file.pdf'); text=''.join(p.get_text() for p in doc); print(f'页数:{len(doc)} 字符:{len(text)} 词数:{len(text.split())}')"

# 批量统计目录下所有 PDF
py -c "import pymupdf,os; d='path/to/pdfs'; [print(f'{f:60s} pages:{len(d):>5} chars:{sum(len(p.get_text()) for p in d):>10,}') for f in sorted(os.listdir(d)) if f.endswith('.pdf') for d in [pymupdf.open(os.path.join(d,f))]]"
```

---

## 2. RAG 召回机制

### 召回流程

```
用户提问 → Embedding模型 → 向量检索 + BM25 → Rerank → 召回结果 → LLM 生成回答
           (非LLM)        (检索系统)         (非LLM)           (LLM介入)
```

**召回（Retrieval）本身不依赖 LLM**，是独立的信息检索过程。LLM 的参与点在于：
- **召回前**：Query Rewrite（Agentic RAG 模式下，LLM 可改写查询提升召回率）
- **召回后**：Relevance Grading（评判片段相关性）和最终回答生成

### 影响召回质量的关键因素（按影响力排序）

| 因素               | 说明                                                     | 本系统现状                            |
| ------------------ | -------------------------------------------------------- | ------------------------------------- |
| **文档切片**       | 切片策略决定检索最小单元。太大→噪声多，太小→语义不完整   | Markdown 语义切片（按标题层级），较好 |
| **Embedding 模型** | 决定语义理解能力，影响领域术语的匹配效果                 | SiliconFlow API                       |
| **检索策略**       | 向量（语义）+ BM25（关键词）混合检索效果最佳             | 已实现混合检索                        |
| **Rerank**         | 对初次召回的候选片段二次排序，过滤"表面相似但无关"的结果 | 已实现                                |
| **Query 改写**     | 优化模糊提问的召回率                                     | Agentic RAG 模式支持                  |

---

## 3. 向量数据库容量对比

| 向量数据库                |  适合片段规模  | 类型       | 备注                      |
| ------------------------- | :------------: | ---------- | ------------------------- |
| **ChromaDB** (本系统在用) | **10 万以内**  | 单机嵌入式 | SQLite 存储，轻量易部署   |
| FAISS                     | 100 万~1000 万 | 单机内存   | Meta 开源，速度快但占内存 |
| Milvus                    |     1 亿+      | 分布式     | 适合大规模生产环境        |
| Pinecone / Weaviate       |     1 亿+      | 云服务     | 按需扩展，免运维          |

### 本系统评估

- 当前 7 个文档，预计总片段数：**1.5~2 万**
- ChromaDB 完全够用，无需升级
- **片段数增多时的真正瓶颈不在存储上限，而在检索精度**：片段越多，语义相近的噪声片段越多，可通过以下方式缓解：
  - 元数据过滤（按文档/章节缩小搜索范围）
  - Rerank 精选最相关结果
  - 合理设置 Top-K 值

---

## 4. AI 上下文窗口 (Context Window)

**Q: 200k 上下文窗口是指 200k 个字符吗？**

**A: 不是，是指 200k Tokens。**

### Token vs 字符换算

| 语言     | 1 Token 约等于            | 200k Tokens 约等于           |
| -------- | ------------------------- | ---------------------------- |
| **英文** | 0.75 个单词 (或 4 个字符) | ~15 万单词 (约 500-600 页书) |
| **中文** | 1.5 ~ 2 个汉字            | ~30-40 万汉字                |

> 注：现代模型（如 GPT-4、Claude 3、DeepSeek）对中文 Tokenization 优化较好，有时可接近 1 Token ≈ 1 汉字。

### 上下文窗口的局限性

虽然模型宣称支持超长上下文（如 200k、1M），但实际应用中存在以下问题：

1. **"大海捞针"效应 (Lost in the Middle)**：当一次性塞入过长文本时，位于中间的信息往往容易被模型忽略，召回准确率下降。
2. **推理成本与速度**：输入越长，推理速度越慢（首字延迟增加），且 API 调用费用成倍增长。
3. **RAG 的优势**：RAG 的核心价值正是不需要将几十万字全部塞入 Context，而是通过检索只提供最相关的几千字（4k-8k tokens），实现**高精度、低延迟、低成本**的回答。
